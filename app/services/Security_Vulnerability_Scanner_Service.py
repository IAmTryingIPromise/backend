"""
Security Vulnerability Scanner Service
Integrates the vulnerability scanning logic with the backend database
"""

import asyncio
import aiohttp
import json
import csv
import os
import re
import time
from mitreattack.stix20 import MitreAttackData
from typing import Dict, List, Optional, Any, Tuple, Set
from functools import lru_cache
from collections import defaultdict
import logging
from fuzzywuzzy import fuzz
from sqlalchemy.orm import Session
from sqlalchemy.exc import IntegrityError

from app.crud import asset as asset_crud
from app.crud import cve as cve_crud
from app.crud import cwe as cwe_crud
from app.crud import capec as capec_crud
from app.crud import attack as attack_crud
from app.crud import relations as relations_crud
from app.schemas.asset import AssetCreate
from app.schemas.cve import CVECreate
from app.schemas.cwe import CWECreate
from app.schemas.capec import CAPECCreate
from app.schemas.attack import AttackCreate
from app.utils.logger import logger

# Suppress HTTP debug logs for cleaner output
logging.getLogger('aiohttp').setLevel(logging.WARNING)


def is_device_vulnerable(cve_item, cpe_name):
    """Check if device is vulnerable to the given CVE."""
    cpe_name = cpe_name.replace('-', '*')
    for configs in cve_item["cve"]["configurations"]:
        for node in configs["nodes"]:
            for cpe_match in node["cpeMatch"]:
                if (cpe_match.get("criteria") == cpe_name and 
                    cpe_match.get("vulnerable") == True):
                    return True
    return False


class OptimizedCPEMatcher:
    """Advanced CPE matcher with fuzzy logic and indexing"""
    
    def __init__(self, devices_data: List[Dict]):
        self.devices_data = devices_data
        self.vendor_index = defaultdict(list)
        self.model_index = defaultdict(list)
        self.token_index = defaultdict(set)
        self.normalized_devices = []
        
        logger.info("Building CPE matching index...")
        self._build_indexes()
        logger.info(f"Index built: {len(self.vendor_index)} vendors, {len(self.model_index)} models, {len(self.token_index)} tokens")
    
    def _build_indexes(self):
        """Build search indexes for faster CPE matching"""
        for idx, device in enumerate(self.devices_data):
            # Normalize device data
            normalized_device = self._normalize_device(device)
            self.normalized_devices.append(normalized_device)
            
            # Index by vendor
            vendor_norm = normalized_device['vendor_normalized']
            if vendor_norm:
                self.vendor_index[vendor_norm].append(idx)
            
            # Index by model components
            model_components = normalized_device['model_components']
            if model_components['base_model']:
                self.model_index[model_components['base_model']].append(idx)
            
            # Index by tokens
            all_tokens = normalized_device['all_tokens']
            for token in all_tokens:
                if len(token) > 2:  # Skip very short tokens
                    self.token_index[token].add(idx)
    
    def _normalize_text(self, text: str) -> str:
        """Enhanced text normalization that handles escaped characters"""
        if not text:
            return ""
        
        # Convert to lowercase
        text = text.lower()
        
        # Handle escaped characters first
        text = text.replace('\\/', '/').replace('\\_', '_').replace('\\-', '-')
        
        # Replace common separators with spaces
        text = text.replace('-', ' ').replace('/', ' ').replace('_', ' ')
        text = text.replace('(', ' ').replace(')', ' ')
        text = text.replace('[', ' ').replace(']', ' ')
        text = text.replace(',', ' ').replace('.', ' ')
        
        # Handle multiple spaces
        text = ' '.join(text.split())
        
        return text
    
    def _extract_model_components(self, model: str) -> Dict[str, any]:
        """Extract components from model string for better matching"""
        if not model:
            return {"base_model": "", "modifiers": [], "full_normalized": "", "original": ""}
        
        normalized = self._normalize_text(model)
        tokens = normalized.split()
        
        base_model = ""
        modifiers = []
        
        # Look for patterns like "1100", "2960x", etc.
        for i, token in enumerate(tokens):
            if re.match(r'^[0-9]+[a-z0-9]*$', token):
                base_model = token
                modifiers = tokens[i+1:]
                break
        
        if not base_model and tokens:
            base_model = tokens[0]
            modifiers = tokens[1:]
        
        return {
            "base_model": base_model,
            "modifiers": modifiers,
            "full_normalized": normalized,
            "original": model.lower()
        }
    
    def _normalize_device(self, device: Dict) -> Dict:
        """Pre-normalize device data for faster searching"""
        vendor = device.get('vendor', '')
        model = device.get('model', '')
        title = device.get('title', '')
        device_type = device.get('type', '')
        
        vendor_normalized = self._normalize_text(vendor)
        model_normalized = self._normalize_text(model)
        title_normalized = self._normalize_text(title)
        type_normalized = self._normalize_text(device_type)
        
        model_components = self._extract_model_components(model)
        
        # Create searchable combinations
        searchable_strings = []
        
        # Basic fields
        if vendor: searchable_strings.append((vendor.lower(), 0.7))
        if model: searchable_strings.append((model.lower(), 1.0))
        if title: searchable_strings.append((title.lower(), 0.9))
        if device_type: searchable_strings.append((device_type.lower(), 0.6))
        
        # Normalized versions
        if vendor_normalized: searchable_strings.append((vendor_normalized, 0.7))
        if model_normalized: searchable_strings.append((model_normalized, 1.0))
        if title_normalized: searchable_strings.append((title_normalized, 0.9))
        if type_normalized: searchable_strings.append((type_normalized, 0.6))
        
        # Combinations
        if vendor and model:
            searchable_strings.extend([
                (f"{vendor.lower()} {model.lower()}", 1.0),
                (f"{vendor_normalized} {model_normalized}", 1.0),
            ])
        
        if vendor_normalized and type_normalized:
            searchable_strings.append((f"{vendor_normalized} {type_normalized}", 0.8))
        
        if model_normalized and type_normalized:
            searchable_strings.append((f"{model_normalized} {type_normalized}", 0.8))
        
        if vendor_normalized and model_normalized and type_normalized:
            searchable_strings.append((f"{vendor_normalized} {model_normalized} {type_normalized}", 0.9))
        
        # Collect all tokens for indexing
        all_tokens = set()
        for text in [vendor_normalized, model_normalized, title_normalized, type_normalized]:
            if text:
                all_tokens.update(text.split())
        
        return {
            'original_device': device,
            'vendor_normalized': vendor_normalized,
            'model_normalized': model_normalized,
            'title_normalized': title_normalized,
            'type_normalized': type_normalized,
            'model_components': model_components,
            'searchable_strings': searchable_strings,
            'all_tokens': all_tokens
        }
    
    def _get_candidate_devices(self, search_query: str) -> Set[int]:
        """Get candidate device indices using indexes for faster filtering"""
        search_normalized = self._normalize_text(search_query)
        search_tokens = set(search_normalized.split())
        search_components = self._extract_model_components(search_query)
        
        candidates = set()
        
        # Search by base model
        if search_components['base_model']:
            base_model = search_components['base_model']
            if base_model in self.model_index:
                candidates.update(self.model_index[base_model])
        
        # Search by vendor (extract potential vendor from search)
        vendor_candidates = set()
        for token in search_tokens:
            if token in self.vendor_index:
                vendor_candidates.update(self.vendor_index[token])
        
        # Search by token overlap
        token_candidates = set()
        for token in search_tokens:
            if token in self.token_index:
                token_candidates.update(self.token_index[token])
        
        # Combine candidates
        if candidates:
            # If we found base model matches, prioritize those
            if vendor_candidates:
                candidates = candidates.intersection(vendor_candidates)
            if not candidates and token_candidates:
                candidates = token_candidates
        else:
            # Fall back to token-based search
            candidates = token_candidates
        
        # If still no candidates, include vendor matches
        if not candidates:
            candidates = vendor_candidates
        
        # If still nothing, fall back to broader token search
        if not candidates and len(search_tokens) > 1:
            for token in search_tokens:
                if len(token) > 3:  # Only longer tokens
                    if token in self.token_index:
                        candidates.update(list(self.token_index[token])[:1000])  # Limit to prevent too many candidates
        
        return candidates
    
    def _calculate_model_similarity(self, search_components: Dict, device_components: Dict) -> float:
        """Calculate similarity between search query and device model components"""
        search_base = search_components["base_model"]
        search_mods = set(search_components["modifiers"])
        device_base = device_components["base_model"]
        device_mods = set(device_components["modifiers"])
        
        if not search_base or not device_base:
            base_score = 0
        else:
            base_score = fuzz.ratio(search_base, device_base)
        
        if base_score < 80:
            return 0
        
        if not search_mods and not device_mods:
            modifier_score = 100
        elif not search_mods or not device_mods:
            modifier_score = 60
        else:
            common_mods = search_mods.intersection(device_mods)
            total_search_mods = len(search_mods)
            
            if total_search_mods == 0:
                modifier_score = 100
            else:
                modifier_score = (len(common_mods) / total_search_mods) * 100
                if search_mods == device_mods:
                    modifier_score += 20
        
        final_score = (base_score * 0.6) + (modifier_score * 0.4)
        return min(final_score, 100)
    
    def find_matching_cpe(self, device_name: str, threshold: int = 70) -> List[str]:
        """Find matching CPE using advanced fuzzy logic"""
        device_name_lower = device_name.lower().strip()
        device_name_norm = self._normalize_text(device_name_lower)
        device_tokens = set(device_name_norm.split())
        
        search_components = self._extract_model_components(device_name)
        
        logger.info(f"Searching for: '{device_name}'")
        logger.info(f"Normalized search: '{device_name_norm}'")
        logger.info(f"Search components: {search_components}")
        
        # Get candidate devices using indexes
        candidate_indices = self._get_candidate_devices(device_name)
        logger.info(f"Filtering {len(candidate_indices)} candidates from {len(self.devices_data)} total devices")
        
        if not candidate_indices:
            logger.info("No candidates found, falling back to broader search...")
            # Fall back to top 5000 devices if no candidates found
            candidate_indices = set(range(min(5000, len(self.devices_data))))
        
        best_matches = []
        matches_found = []
        
        for idx in candidate_indices:
            if idx >= len(self.normalized_devices):
                continue
                
            normalized_device = self.normalized_devices[idx]
            device = normalized_device['original_device']
            
            # Quick vendor validation
            vendor_tokens = set(normalized_device['vendor_normalized'].split()) if normalized_device['vendor_normalized'] else set()
            vendor_match = bool(vendor_tokens.intersection(device_tokens)) if vendor_tokens else True
            
            # Calculate model component similarity
            device_components = normalized_device['model_components']
            model_similarity = self._calculate_model_similarity(search_components, device_components)
            
            device_score = 0
            best_search_string = ""
            
            # Test against pre-computed searchable strings
            for search_string, weight in normalized_device['searchable_strings']:
                search_string_norm = self._normalize_text(search_string)
                search_tokens = set(search_string_norm.split())
                
                # Token overlap score
                token_overlap = len(device_tokens.intersection(search_tokens))
                total_tokens = len(device_tokens.union(search_tokens))
                token_overlap_score = (token_overlap / total_tokens * 100) if total_tokens > 0 else 0
                
                # Fuzzy scores (reduced set for speed)
                fuzzy_scores = [
                    fuzz.ratio(device_name_norm, search_string_norm),
                    fuzz.token_sort_ratio(device_name_norm, search_string_norm),
                    token_overlap_score
                ]
                
                max_fuzzy_score = max(fuzzy_scores)
                combined_score = (max_fuzzy_score * 0.6) + (model_similarity * 0.4)
                weighted_score = combined_score * weight
                
                # Vendor bonus/penalty
                if vendor_tokens:
                    if vendor_match:
                        weighted_score += 10
                    else:
                        weighted_score -= 15
                
                if weighted_score > device_score:
                    device_score = weighted_score
                    best_search_string = search_string
            
            matches_found.append({
                'device': device,
                'score': device_score,
                'model_similarity': model_similarity,
                'vendor_match': vendor_match,
                'search_string': best_search_string
            })
            
            if device_score >= threshold:
                cpe = device.get('cpeName', '').replace("\\/", "/")
                if cpe:
                    best_matches.append((cpe, device_score))
        
        # Sort matches by score
        matches_found.sort(key=lambda x: x['score'], reverse=True)
        best_matches.sort(key=lambda x: x[1], reverse=True)
        
        # Show top candidates
        logger.info("\nTop candidates found:")
        for i, match in enumerate(matches_found[:10], 1):
            device = match['device']
            logger.info(f"{i}. {device.get('model', 'Unknown')} (Score: {match['score']:.1f}, "
                  f"Model Sim: {match['model_similarity']:.1f}, "
                  f"Vendor: {device.get('vendor', 'Unknown')})")
        
        # Return CPEs of best matches
        matched_cpes = [cpe for cpe, score in best_matches]
        
        if matched_cpes:
            logger.info(f"\nFound {len(matched_cpes)} matching CPEs:")
            for i, cpe in enumerate(matched_cpes[:5], 1):  # Show top 5
                logger.info(f"{i}. {cpe}")
        else:
            logger.info(f"No CPE found matching '{device_name}' with threshold {threshold}")
        
        return matched_cpes


class SecurityDataFetcher:
    def __init__(self, api_key: str, max_concurrent: int = 25):
        self.api_key = api_key
        self.max_concurrent = max_concurrent
        self.session: Optional[aiohttp.ClientSession] = None
        self.semaphore = asyncio.Semaphore(max_concurrent)
        
        # Batch cache for CWE details to avoid duplicate API calls
        self.cwe_cache: Dict[str, Any] = {}
        
    async def __aenter__(self):
        # Optimized connector settings for maximum performance
        connector = aiohttp.TCPConnector(
            limit=100,           # Total connection pool size
            limit_per_host=50,   # Connections per host
            ttl_dns_cache=300,   # DNS cache TTL
            use_dns_cache=True,
            keepalive_timeout=30,
            enable_cleanup_closed=True
        )
        timeout = aiohttp.ClientTimeout(total=20, connect=5)
        self.session = aiohttp.ClientSession(
            connector=connector, 
            timeout=timeout,
            headers={'User-Agent': 'SecurityScanner/1.0'}
        )
        return self
        
    async def __aexit__(self, exc_type, exc_val, exc_tb):
        if self.session:
            await self.session.close()

    async def fetch_cwe_details_batch(self, cwe_ids: List[str]) -> Dict[str, Any]:
        """Fetch multiple CWE details concurrently with smart batching."""
        # Filter out already cached CWEs
        uncached_cwe_ids = [cwe_id for cwe_id in cwe_ids if cwe_id not in self.cwe_cache]
        
        if not uncached_cwe_ids:
            return {cwe_id: self.cwe_cache[cwe_id] for cwe_id in cwe_ids}
        
        logger.info(f"Fetching {len(uncached_cwe_ids)} unique CWE details...")
        
        # Create tasks for uncached CWEs
        tasks = []
        for cwe_id in uncached_cwe_ids:
            task = self._fetch_single_cwe(cwe_id)
            tasks.append(task)
        
        # Execute all CWE requests concurrently
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # Update cache with results
        for cwe_id, result in zip(uncached_cwe_ids, results):
            if isinstance(result, Exception):
                self.cwe_cache[cwe_id] = None
            else:
                self.cwe_cache[cwe_id] = result
        
        # Return all requested CWEs (cached + newly fetched)
        return {cwe_id: self.cwe_cache[cwe_id] for cwe_id in cwe_ids}

    async def _fetch_single_cwe(self, cwe_id: str) -> Optional[Dict]:
        """Fetch a single CWE with semaphore control."""
        url = f"https://cwe-api.mitre.org/api/v1/cwe/weakness/{cwe_id}"
        
        async with self.semaphore:
            try:
                async with self.session.get(url) as response:
                    if response.status == 200:
                        return await response.json()
                    return None
            except Exception:
                return None

    async def fetch_epss_scores_batch(self, cve_ids: List[str]) -> Dict[str, float]:
        """Fetch EPSS scores for multiple CVEs in batches."""
        logger.info(f"Fetching EPSS scores for {len(cve_ids)} CVEs...")
        
        # EPSS API supports multiple CVEs in one request
        batch_size = 50  # EPSS API limit
        all_results = {}
        
        tasks = []
        for i in range(0, len(cve_ids), batch_size):
            batch = cve_ids[i:i + batch_size]
            task = self._fetch_epss_batch(batch)
            tasks.append(task)
        
        batch_results = await asyncio.gather(*tasks, return_exceptions=True)
        
        for result in batch_results:
            if isinstance(result, dict):
                all_results.update(result)
        
        return all_results

    async def _fetch_epss_batch(self, cve_batch: List[str]) -> Dict[str, float]:
        """Fetch EPSS scores for a batch of CVEs."""
        url = "https://api.first.org/data/v1/epss"
        params = {"cve": ",".join(cve_batch)}
        
        async with self.semaphore:
            try:
                async with self.session.get(url, params=params) as response:
                    if response.status == 200:
                        data = await response.json()
                        results = {}
                        for item in data.get("data", []):
                            results[item["cve"]] = float(item["epss"])
                        return results
            except Exception as e:
                logger.error(f"Error fetching EPSS batch: {e}")
            
            return {}

    async def fetch_nvd_data(self, cpe_name: str) -> Dict:
        """Fetch CVE data from NVD with optimized parameters."""
        url = "https://services.nvd.nist.gov/rest/json/cves/2.0"
        params = {
            "cpeName": cpe_name,
            "startIndex": 0,
            "resultsPerPage": 2000,  # Fetch more results per request
        }
        headers = {"apiKey": self.api_key}
        
        logger.info("Fetching NVD data...")
        async with self.semaphore:
            try:
                async with self.session.get(url, params=params, headers=headers) as response:
                    if response.status == 200:
                        return await response.json()
                    else:
                        logger.error(f"NVD API Error: {response.status}")
                        return {}
            except Exception as e:
                logger.error(f"Error fetching NVD data: {e}")
                return {}


class OptimizedDataProcessor:
    def __init__(self, script_path: str):
        self.script_path = script_path
        self.load_static_data()
        
        # Pre-build lookup indexes for faster searches
        self._build_capec_index()
        
        # Initialize the enhanced CPE matcher
        self.cpe_matcher = OptimizedCPEMatcher(self.cpes)
        
    def load_static_data(self):
        """Load static data files once at initialization with optimized parsing."""
        logger.info("Loading static data files...")
        
        # Load device CPEs
        device_cpes_path = os.path.join(self.script_path, "hardware_devices.json")
        with open(device_cpes_path, 'r', encoding='utf-8') as f:
            self.cpes = json.load(f)
            
        # Load CAPEC CSV data with optimized parsing
        capec_csv_path = os.path.join(self.script_path, "2000CAPEC.csv")
        self.capec_data = []
        try:
            with open(capec_csv_path, 'r', encoding='utf-8') as csvfile:
                # Use faster csv parsing
                reader = csv.DictReader(csvfile)
                self.capec_data = list(reader)
                self.capec_fieldnames = reader.fieldnames
        except FileNotFoundError:
            logger.error(f"CAPEC CSV file not found: {capec_csv_path}")
            
        # Load ATT&CK data
        attack_json_path = os.path.join(self.script_path, "enterprise-attack.json")
        self.attack_data = MitreAttackData(attack_json_path)
        
        # Headers for CAPEC display
        self.headers_to_show = {
            "'ID", "Name", "Description", "Likelihood Of Attack", 
            "Typical Severity", "Related Weaknesses", "Mitigations",
            "Prerequisites", "Consequences", "Example Instances"
        }

    def _build_capec_index(self):
        """Pre-build an index of CAPEC data by CWE for O(1) lookups."""
        logger.info("Building CAPEC lookup index...")
        self.capec_by_cwe = defaultdict(list)
        
        for row in self.capec_data:
            weakness_data = row.get("Related Weaknesses", "").strip()
            if weakness_data:
                # Extract CWE numbers
                cwe_numbers = re.findall(r'::(\d+)::', weakness_data)
                for cwe_id in cwe_numbers:
                    self.capec_by_cwe[cwe_id].append({
                        "capec_data": {header: row.get(header, "") for header in self.headers_to_show},
                        "taxonomy_mappings": row.get("Taxonomy Mappings", "")
                    })

    def find_matching_cpe(self, input_string: str) -> List[str]:
        """Find matching CPE using enhanced fuzzy logic"""
        return self.cpe_matcher.find_matching_cpe(input_string)

    def process_vulnerabilities_batch(self, vulnerabilities: List[Dict], cpe_name) -> Tuple[List[Dict], List[str], Dict[str, List[str]]]:
        """Process all vulnerabilities and extract data in batches for optimal performance."""
        logger.info(f"Processing {len(vulnerabilities)} vulnerabilities...")
        
        processed_cves = []
        all_cve_ids = []
        cve_to_cwes = {}
        all_unique_cwes = set()
        
        for cve_item in vulnerabilities:

            if not is_device_vulnerable(cve_item, cpe_name):
                continue

            cve_id = cve_item["cve"]["id"]
            description = cve_item["cve"]["descriptions"][0]["value"]
            
            # Extract metrics
            metrics = self.extract_cve_metrics(cve_item)
            
            # Extract CWEs
            cwe_ids = self.extract_cwes(cve_item)
            
            processed_cves.append({
                "cve_item": cve_item,
                "cve_id": cve_id,
                "description": description,
                "metrics": metrics,
                "cwe_ids": cwe_ids
            })
            
            all_cve_ids.append(cve_id)
            cve_to_cwes[cve_id] = cwe_ids
            all_unique_cwes.update(cwe_ids)
        
        logger.info(f"Found {len(processed_cves)} vulnerable CVEs for the device")
        return processed_cves, all_cve_ids, cve_to_cwes

    def extract_cve_metrics(self, cve_item: Dict) -> Dict:
        """Extract CVSS metrics from CVE data."""
        metrics = cve_item.get("cve", {}).get("metrics", {})
        
        for version in ["cvssMetricV31", "cvssMetricV30", "cvssMetricV2"]:
            if version in metrics:
                metric_data = metrics[version][0]
                return {
                    "cvss": metric_data["cvssData"]["baseScore"],
                    "impact": metric_data["impactScore"],
                    "exploitability": metric_data["exploitabilityScore"]
                }
        
        return {"cvss": 0, "impact": 0, "exploitability": 0}

    def extract_cwes(self, cve_item: Dict) -> List[str]:
        """Extract CWE IDs from CVE data."""
        cwes = []
        for weakness in cve_item["cve"].get("weaknesses", []):
            for desc in weakness.get("description", []):
                cwe_id = desc.get("value", "").split("-")[-1]
                if cwe_id and cwe_id != "noinfo":
                    cwes.append(cwe_id)
        return list(set(cwes))  # Remove duplicates

    def find_related_capec_fast(self, cwe_id: str) -> List[Dict]:
        """Fast CAPEC lookup using pre-built index."""
        return self.capec_by_cwe.get(cwe_id, [])

    @lru_cache(maxsize=500)
    def extract_attack_ids(self, taxonomy_data: str) -> Tuple[str, ...]:
        """Extract MITRE ATT&CK technique IDs with caching."""
        found_entry_ids = re.findall(r'ENTRY ID:([^:]+)', taxonomy_data)
        return tuple(f"T{entry_id}" for entry_id in found_entry_ids)


class VulnerabilityScanner:
    """Main vulnerability scanner service that integrates with the database"""
    
    def __init__(self, api_key: str, script_path: str):
        self.api_key = api_key
        self.script_path = script_path
        self.processor = OptimizedDataProcessor(script_path)
        
    async def scan_device(self, device_name: str, department: str, db_session: Session) -> Dict[str, Any]:
        """
        Main scanning method that processes a device and stores results in database
        """
        start_time = time.time()
        
        try:
            # Find matching CPE
            cpe_matches = self.processor.find_matching_cpe(device_name)
            if not cpe_matches:
                return {
                    "device_name": device_name,
                    "department": department,
                    "cpe_matches": [],
                    "vulnerabilities_found": 0,
                    "cves_processed": 0,
                    "cwes_processed": 0,
                    "capecs_processed": 0,
                    "attacks_processed": 0,
                    "scan_time": time.time() - start_time,
                    "success": False,
                    "error_message": "No matching CPE found for the device"
                }
            
            cpe_name = cpe_matches[0]
            logger.info(f"Using CPE: {cpe_name}")
            
            # Parse device information from CPE and device name
            device_info = self._parse_device_info(device_name, cpe_name, department)
            
            # Create or get asset
            asset = await self._create_or_update_asset(db_session, device_info)
            
            async with SecurityDataFetcher(self.api_key, max_concurrent=30) as fetcher:
                # Fetch NVD data
                nvd_data = await fetcher.fetch_nvd_data(cpe_name)
                
                if not nvd_data.get("vulnerabilities"):
                    logger.info("No vulnerabilities found")
                    return {
                        "device_name": device_name,
                        "department": department,
                        "cpe_matches": cpe_matches,
                        "vulnerabilities_found": 0,
                        "cves_processed": 0,
                        "cwes_processed": 0,
                        "capecs_processed": 0,
                        "attacks_processed": 0,
                        "scan_time": time.time() - start_time,
                        "success": True,
                        "error_message": None
                    }
                
                vulnerabilities = nvd_data["vulnerabilities"]
                logger.info(f"Found {len(vulnerabilities)} vulnerabilities")
                
                # Process all vulnerabilities at once to extract CVE IDs and CWEs
                processed_cves, all_cve_ids, cve_to_cwes = self.processor.process_vulnerabilities_batch(
                    vulnerabilities, cpe_name
                )
                unique_cwes = list(set(cwe_id for cwe_list in cve_to_cwes.values() for cwe_id in cwe_list))
                
                # Fetch all external data concurrently
                logger.info("Fetching all external data concurrently...")
                epss_task = fetcher.fetch_epss_scores_batch(all_cve_ids)
                cwe_task = fetcher.fetch_cwe_details_batch(unique_cwes)
                
                # Wait for all external data
                epss_scores, all_cwe_details = await asyncio.gather(epss_task, cwe_task)
                
                # Process and store all data
                cves_processed = 0
                cwes_processed = 0
                capecs_processed = 0
                attacks_processed = 0
                
                # Store CVEs and their relationships
                for cve_data in processed_cves:
                    cve_id = cve_data["cve_id"]
                    description = cve_data["description"]
                    metrics = cve_data["metrics"]
                    cwe_ids = cve_data["cwe_ids"]
                    
                    # Get EPSS score
                    epss_score = epss_scores.get(cve_id, 0)
                    
                    # Calculate risk level
                    if epss_score:
                        risk_level = epss_score * 1000 * metrics["impact"] * metrics["exploitability"]
                    else:
                        risk_level = 0
                    
                    # Create CVE in database
                    cve_db = await self._create_or_get_cve(
                        db_session, cve_id, description, metrics["cvss"], 
                        risk_level, epss_score, metrics["impact"], metrics["exploitability"]
                    )
                    
                    if cve_db:
                        cves_processed += 1
                        
                        # Create asset-CVE relationship
                        await self._create_asset_cve_relation(db_session, asset.id, cve_db.id)
                        
                        # Process CWEs for this CVE
                        for cwe_id in cwe_ids:
                            cwe_details = all_cwe_details.get(cwe_id)
                            
                            # Create CWE in database
                            cwe_db = await self._create_or_get_cwe(db_session, cwe_id, cwe_details)
                            
                            if cwe_db:
                                cwes_processed += 1
                                
                                # Create CVE-CWE relationship
                                await self._create_cve_cwe_relation(db_session, cve_db.id, cwe_db.id)
                                
                                # Process CAPECs for this CWE
                                related_capecs = self.processor.find_related_capec_fast(cwe_id)
                                
                                for capec_info in related_capecs:
                                    capec_data = capec_info["capec_data"]
                                    taxonomy_mappings = capec_info["taxonomy_mappings"]
                                    
                                    # Create CAPEC in database
                                    capec_db = await self._create_or_get_capec(db_session, capec_data)
                                    
                                    if capec_db:
                                        capecs_processed += 1
                                        
                                        # Create CWE-CAPEC relationship
                                        await self._create_cwe_capec_relation(db_session, cwe_db.id, capec_db.id)
                                        
                                        # Process ATT&CK techniques for this CAPEC
                                        attack_ids = self.processor.extract_attack_ids(taxonomy_mappings)
                                        
                                        for attack_id in attack_ids:
                                            technique = self.processor.attack_data.get_object_by_attack_id(
                                                attack_id, "attack-pattern"
                                            )
                                            
                                            if technique:
                                                # Create ATT&CK in database
                                                attack_db = await self._create_or_get_attack(
                                                    db_session, attack_id, technique
                                                )
                                                
                                                if attack_db:
                                                    attacks_processed += 1
                                                    
                                                    # Create CAPEC-Attack relationship
                                                    await self._create_capec_attack_relation(
                                                        db_session, capec_db.id, attack_db.id
                                                    )
                
                # Update asset risk level based on all CVEs
                await self._update_asset_risk_level(db_session, asset.id)
                
                end_time = time.time()
                scan_time = end_time - start_time
                
                return {
                    "device_name": device_name,
                    "department": department,
                    "cpe_matches": cpe_matches,
                    "vulnerabilities_found": len(vulnerabilities),
                    "cves_processed": cves_processed,
                    "cwes_processed": cwes_processed,
                    "capecs_processed": capecs_processed,
                    "attacks_processed": attacks_processed,
                    "scan_time": scan_time,
                    "success": True,
                    "error_message": None
                }
                
        except Exception as e:
            logger.error(f"Error scanning device {device_name}: {str(e)}")
            return {
                "device_name": device_name,
                "department": department,
                "cpe_matches": [],
                "vulnerabilities_found": 0,
                "cves_processed": 0,
                "cwes_processed": 0,
                "capecs_processed": 0,
                "attacks_processed": 0,
                "scan_time": time.time() - start_time,
                "success": False,
                "error_message": str(e)
            }
    
    def _parse_device_info(self, device_name: str, cpe_name: str, department: str) -> Dict[str, Any]:
        """Parse device information from device name and CPE"""
        # Extract vendor, model, version from CPE
        # CPE format: cpe:2.3:h:vendor:product:version:update:edition:language:sw_edition:target_sw:target_hw:other
        cpe_parts = cpe_name.split(":")
        
        vendor = ""
        model = ""
        version = ""
        device_type = "hardware"
        
        if len(cpe_parts) >= 4:
            vendor = cpe_parts[3].replace("_", " ").title()
        if len(cpe_parts) >= 5:
            model = cpe_parts[4].replace("_", " ")
        if len(cpe_parts) >= 6:
            version = cpe_parts[5].replace("_", ".")
        
        # If we can't parse from CPE, try to extract from device name
        if not vendor or not model:
            # Simple heuristic: first word is often vendor
            parts = device_name.split()
            if parts:
                if not vendor:
                    vendor = parts[0]
                if not model and len(parts) > 1:
                    model = " ".join(parts[1:])
        
        return {
            "name": device_name,
            "vendor": vendor,
            "model": model,
            "version": version,
            "type": device_type,
            "department": department,
            "description": f"{vendor} {model} {version}".strip(),
            "risk_level": 0.0  # Will be calculated after processing vulnerabilities
        }
    
    async def _create_or_update_asset(self, db_session: Session, device_info: Dict[str, Any]):
        """Create or update asset in database"""
        try:
            # Check if asset already exists by name
            existing_asset = db_session.query(asset_crud.Asset.__table__.c.name == device_info["name"]).first()
            
            if existing_asset:
                # Update existing asset
                asset_update_data = {k: v for k, v in device_info.items() if k != "name"}
                asset = asset_crud.update_asset(db_session, existing_asset.id, asset_update_data)
            else:
                # Create new asset
                asset_create = AssetCreate(**device_info)
                asset = asset_crud.create_asset(db_session, asset_create)
            
            return asset
        except Exception as e:
            logger.error(f"Error creating/updating asset: {e}")
            raise
    
    async def _create_or_get_cve(self, db_session: Session, cve_id: str, description: str, 
                                cvss: float, risk_level: float, epss: float, 
                                impact: float, exploitability: float):
        """Create or get CVE in database"""
        try:
            # Check if CVE already exists
            existing_cve = cve_crud.get_cve_by_cve_id(db_session, cve_id)
            
            if existing_cve:
                return existing_cve
            
            # Create new CVE
            cve_create = CVECreate(
                cve_id=cve_id,
                description=description,
                cvss=cvss,
                risk_level=risk_level,
                epss=epss,
                impact_score=impact,
                exploitability_score=exploitability
            )
            
            return cve_crud.create_cve(db_session, cve_create)
            
        except IntegrityError:
            # CVE might have been created by another thread
            db_session.rollback()
            return cve_crud.get_cve_by_cve_id(db_session, cve_id)
        except Exception as e:
            logger.error(f"Error creating CVE {cve_id}: {e}")
            return None
    
    async def _create_or_get_cwe(self, db_session: Session, cwe_id: str, cwe_details: Dict):
        """Create or get CWE in database"""
        try:
            # Check if CWE already exists
            existing_cwe = cwe_crud.get_cwe_by_cwe_id(db_session, cwe_id)
            
            if existing_cwe:
                return existing_cwe
            
            # Extract CWE information
            name = "Unknown"
            description = "No description available"
            
            if cwe_details and "Weaknesses" in cwe_details:
                weakness = cwe_details["Weaknesses"][0]
                name = weakness.get("Name", "Unknown")
                description = weakness.get("Description", "No description available")
            
            # Create new CWE
            cwe_create = CWECreate(
                cwe_id=cwe_id,
                name=name,
                description=description
            )
            
            return cwe_crud.create_cwe(db_session, cwe_create)
            
        except IntegrityError:
            # CWE might have been created by another thread
            db_session.rollback()
            return cwe_crud.get_cwe_by_cwe_id(db_session, cwe_id)
        except Exception as e:
            logger.error(f"Error creating CWE {cwe_id}: {e}")
            return None
    
    async def _create_or_get_capec(self, db_session: Session, capec_data: Dict):
        """Create or get CAPEC in database"""
        try:
            capec_id = capec_data.get("'ID", "").replace("CAPEC-", "")
            if not capec_id:
                return None
            
            # Check if CAPEC already exists
            existing_capec = capec_crud.get_capec_by_capec_id(db_session, capec_id)
            
            if existing_capec:
                return existing_capec
            
            # Create new CAPEC
            capec_create = CAPECCreate(
                capec_id=capec_id,
                name=capec_data.get("Name", "Unknown"),
                description=capec_data.get("Description", "No description available"),
                typical_severity=capec_data.get("Typical Severity", "Unknown"),
                likelihood_of_attack=capec_data.get("Likelihood Of Attack", "Unknown")
            )
            
            return capec_crud.create_capec(db_session, capec_create)
            
        except IntegrityError:
            # CAPEC might have been created by another thread
            db_session.rollback()
            return capec_crud.get_capec_by_capec_id(db_session, capec_id)
        except Exception as e:
            logger.error(f"Error creating CAPEC {capec_id}: {e}")
            return None
    
    async def _create_or_get_attack(self, db_session: Session, attack_id: str, technique: Dict):
        """Create or get ATT&CK technique in database"""
        try:
            # Check if Attack already exists
            existing_attack = attack_crud.get_attack_by_technique_id(db_session, attack_id)
            
            if existing_attack:
                return existing_attack
            
            # Extract external references
            external_id = None
            url = None
            for ref in technique.get("external_references", []):
                if ref.get("source_name") == "mitre-attack":
                    external_id = ref.get("external_id")
                    url = ref.get("url")
                    break
            
            # Extract tactics
            tactics = [phase.get("phase_name") for phase in technique.get("kill_chain_phases", [])]
            
            # Create new Attack
            attack_create = AttackCreate(
                technique_id=attack_id,
                external_id=external_id or attack_id,
                name=technique.get("name", "Unknown"),
                description=technique.get("description", "No description available"),
                url=url,
                tactics=", ".join(tactics) if tactics else None,
                platforms=", ".join(technique.get("x_mitre_platforms", [])) if technique.get("x_mitre_platforms") else None,
                data_sources=", ".join(technique.get("x_mitre_data_sources", [])) if technique.get("x_mitre_data_sources") else None,
                detection=technique.get("x_mitre_detection"),
                permissions_required=", ".join(technique.get("x_mitre_permissions_required", [])) if technique.get("x_mitre_permissions_required") else None
            )
            
            return attack_crud.create_attack(db_session, attack_create)
            
        except IntegrityError:
            # Attack might have been created by another thread
            db_session.rollback()
            return attack_crud.get_attack_by_technique_id(db_session, attack_id)
        except Exception as e:
            logger.error(f"Error creating Attack {attack_id}: {e}")
            return None
    
    async def _create_asset_cve_relation(self, db_session: Session, asset_id: int, cve_id: int):
        """Create asset-CVE relationship"""
        try:
            relations_crud.create_asset_cve_relation(db_session, asset_id, cve_id)
        except Exception as e:
            logger.error(f"Error creating asset-CVE relation: {e}")
    
    async def _create_cve_cwe_relation(self, db_session: Session, cve_id: int, cwe_id: int):
        """Create CVE-CWE relationship"""
        try:
            relations_crud.create_cve_cwe_relation(db_session, cve_id, cwe_id)
        except Exception as e:
            logger.error(f"Error creating CVE-CWE relation: {e}")
    
    async def _create_cwe_capec_relation(self, db_session: Session, cwe_id: int, capec_id: int):
        """Create CWE-CAPEC relationship"""
        try:
            relations_crud.create_cwe_capec_relation(db_session, cwe_id, capec_id)
        except Exception as e:
            logger.error(f"Error creating CWE-CAPEC relation: {e}")
    
    async def _create_capec_attack_relation(self, db_session: Session, capec_id: int, attack_id: int):
        """Create CAPEC-Attack relationship"""
        try:
            relations_crud.create_capec_attack_relation(db_session, capec_id, attack_id)
        except Exception as e:
            logger.error(f"Error creating CAPEC-Attack relation: {e}")
    
    async def _update_asset_risk_level(self, db_session: Session, asset_id: int):
        """Update asset risk level based on associated CVEs"""
        try:
            cves = cve_crud.get_cves_by_asset_id(db_session, asset_id)
            
            if not cves:
                return
            
            # Calculate overall risk level (max risk + average risk weighted)
            risk_levels = [cve.risk_level or 0 for cve in cves]
            max_risk = max(risk_levels) if risk_levels else 0
            avg_risk = sum(risk_levels) / len(risk_levels) if risk_levels else 0
            
            # Weighted risk calculation
            overall_risk = (max_risk * 0.7) + (avg_risk * 0.3)
            
            # Update asset
            asset = asset_crud.get_asset(db_session, asset_id)
            if asset:
                asset.risk_level = overall_risk
                db_session.commit()
                
        except Exception as e:
            logger.error(f"Error updating asset risk level: {e}")
            db_session.rollback()